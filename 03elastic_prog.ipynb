{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Complete a program to display documents in the tf-idf vector model.\n",
    "- Compute document similarities with the cosine measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step. \n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the toy English phrase\n",
    "\n",
    "```\n",
    "my taylor 4ís was% &printing printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "\n",
    "\n",
    "client = Elasticsearch(timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `whitespace` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('whitespace'), #all simbols separated with space\n",
    "    filter=['lowercase']\n",
    ")\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index('foo', using=client)\n",
    "ind.settings(number_of_shards=1)\n",
    "# Shards are basically used to parallelize work on an index. When you send a bulk request to index \n",
    "# a list of documents, they will be split and divided among all available primary shards.\n",
    "try:\n",
    "    # drop if exists\n",
    "    ind.delete()\n",
    "except NotFoundError:\n",
    "    pass\n",
    "\n",
    "# create it\n",
    "ind.create()\n",
    "\n",
    "# close to update analyzer to custom `my_analyzer`    \n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was%', 'start_offset': 14, 'end_offset': 18, 'type': 'word', 'position': 3}\n",
      "{'token': '&printing', 'start_offset': 19, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the.', 'start_offset': 42, 'end_offset': 46, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my Taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `standard` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('standard'), #not symbols\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': '<ALPHANUM>', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': '4ís', 'start_offset': 10, 'end_offset': 13, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': '<ALPHANUM>', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': '<ALPHANUM>', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token `letter` filter `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'), # not number nor symbols\n",
    "    filter=['lowercase']\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'ís', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding'] #delete accents\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'is', 'start_offset': 11, 'end_offset': 13, 'type': 'word', 'position': 2}\n",
      "{'token': 'was', 'start_offset': 14, 'end_offset': 17, 'type': 'word', 'position': 3}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n",
      "{'token': 'the', 'start_offset': 42, 'end_offset': 45, 'type': 'word', 'position': 7}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding', 'stop'] # only relevant words\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'printing', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'printed', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `asciifolding` + `stop` + `snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['lowercase','asciifolding','stop', 'snowball'] # snowball is stemming, erasing derivations\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can ask the index to analyze any text, feel free to change the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'my', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'taylor', 'start_offset': 3, 'end_offset': 9, 'type': 'word', 'position': 1}\n",
      "{'token': 'print', 'start_offset': 20, 'end_offset': 28, 'type': 'word', 'position': 4}\n",
      "{'token': 'print', 'start_offset': 29, 'end_offset': 36, 'type': 'word', 'position': 5}\n",
      "{'token': 'rich', 'start_offset': 37, 'end_offset': 41, 'type': 'word', 'position': 6}\n"
     ]
    }
   ],
   "source": [
    "res = ind.analyze(body={'analyzer':'default', 'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for r in res['tokens']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1** using ElasticSearch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'We', 'start_offset': 0, 'end_offset': 2, 'type': 'word', 'position': 0}\n",
      "{'token': 'found', 'start_offset': 3, 'end_offset': 8, 'type': 'word', 'position': 1}\n",
      "{'token': 'my', 'start_offset': 9, 'end_offset': 11, 'type': 'word', 'position': 2}\n",
      "{'token': 'lady', 'start_offset': 12, 'end_offset': 16, 'type': 'word', 'position': 3}\n",
      "{'token': 'light', 'start_offset': 25, 'end_offset': 30, 'type': 'word', 'position': 6}\n",
      "{'token': 'room', 'start_offset': 38, 'end_offset': 42, 'type': 'word', 'position': 9}\n",
      "{'token': 'reading', 'start_offset': 51, 'end_offset': 58, 'type': 'word', 'position': 12}\n",
      "{'token': 'lamp', 'start_offset': 59, 'end_offset': 63, 'type': 'word', 'position': 13}\n",
      "{'token': 'The', 'start_offset': 65, 'end_offset': 68, 'type': 'word', 'position': 14}\n",
      "{'token': 'shade', 'start_offset': 69, 'end_offset': 74, 'type': 'word', 'position': 15}\n",
      "{'token': 'screw', 'start_offset': 79, 'end_offset': 86, 'type': 'word', 'position': 17}\n",
      "{'token': 'down', 'start_offset': 87, 'end_offset': 91, 'type': 'word', 'position': 18}\n",
      "{'token': 'so', 'start_offset': 92, 'end_offset': 94, 'type': 'word', 'position': 19}\n",
      "{'token': 'over', 'start_offset': 101, 'end_offset': 105, 'type': 'word', 'position': 22}\n",
      "{'token': 'shadow', 'start_offset': 106, 'end_offset': 112, 'type': 'word', 'position': 23}\n",
      "{'token': 'her', 'start_offset': 113, 'end_offset': 116, 'type': 'word', 'position': 24}\n",
      "{'token': 'face', 'start_offset': 117, 'end_offset': 121, 'type': 'word', 'position': 25}\n",
      "{'token': 'Instead', 'start_offset': 123, 'end_offset': 130, 'type': 'word', 'position': 26}\n",
      "{'token': 'look', 'start_offset': 134, 'end_offset': 141, 'type': 'word', 'position': 28}\n",
      "{'token': 'up', 'start_offset': 142, 'end_offset': 144, 'type': 'word', 'position': 29}\n",
      "{'token': 'us', 'start_offset': 148, 'end_offset': 150, 'type': 'word', 'position': 31}\n",
      "{'token': 'her', 'start_offset': 154, 'end_offset': 157, 'type': 'word', 'position': 33}\n",
      "{'token': 'usual', 'start_offset': 158, 'end_offset': 163, 'type': 'word', 'position': 34}\n",
      "{'token': 'straightforward', 'start_offset': 164, 'end_offset': 179, 'type': 'word', 'position': 35}\n",
      "{'token': 'way', 'start_offset': 180, 'end_offset': 183, 'type': 'word', 'position': 36}\n",
      "{'token': 'she', 'start_offset': 185, 'end_offset': 188, 'type': 'word', 'position': 37}\n",
      "{'token': 'sat', 'start_offset': 189, 'end_offset': 192, 'type': 'word', 'position': 38}\n",
      "{'token': 'close', 'start_offset': 193, 'end_offset': 198, 'type': 'word', 'position': 39}\n",
      "{'token': 'table', 'start_offset': 206, 'end_offset': 211, 'type': 'word', 'position': 42}\n",
      "{'token': 'kept', 'start_offset': 217, 'end_offset': 221, 'type': 'word', 'position': 44}\n",
      "{'token': 'her', 'start_offset': 222, 'end_offset': 225, 'type': 'word', 'position': 45}\n",
      "{'token': 'eyes', 'start_offset': 226, 'end_offset': 230, 'type': 'word', 'position': 46}\n",
      "{'token': 'fixed', 'start_offset': 231, 'end_offset': 236, 'type': 'word', 'position': 47}\n",
      "{'token': 'obstinate', 'start_offset': 237, 'end_offset': 248, 'type': 'word', 'position': 48}\n",
      "{'token': 'open', 'start_offset': 255, 'end_offset': 259, 'type': 'word', 'position': 51}\n",
      "{'token': 'book', 'start_offset': 260, 'end_offset': 264, 'type': 'word', 'position': 52}\n",
      "{'token': 'Officer', 'start_offset': 267, 'end_offset': 274, 'type': 'word', 'position': 53}\n",
      "{'token': 'she', 'start_offset': 277, 'end_offset': 280, 'type': 'word', 'position': 54}\n",
      "{'token': 'said', 'start_offset': 281, 'end_offset': 285, 'type': 'word', 'position': 55}\n",
      "{'token': 'important', 'start_offset': 294, 'end_offset': 303, 'type': 'word', 'position': 58}\n",
      "{'token': 'inquiry', 'start_offset': 311, 'end_offset': 318, 'type': 'word', 'position': 61}\n",
      "{'token': 'you', 'start_offset': 319, 'end_offset': 322, 'type': 'word', 'position': 62}\n",
      "{'token': 'conduct', 'start_offset': 327, 'end_offset': 337, 'type': 'word', 'position': 64}\n",
      "{'token': 'know', 'start_offset': 341, 'end_offset': 345, 'type': 'word', 'position': 66}\n",
      "{'token': 'beforehand', 'start_offset': 346, 'end_offset': 356, 'type': 'word', 'position': 67}\n",
      "{'token': 'any', 'start_offset': 360, 'end_offset': 363, 'type': 'word', 'position': 69}\n",
      "{'token': 'person', 'start_offset': 364, 'end_offset': 370, 'type': 'word', 'position': 70}\n",
      "{'token': 'now', 'start_offset': 371, 'end_offset': 374, 'type': 'word', 'position': 71}\n",
      "{'token': 'house', 'start_offset': 383, 'end_offset': 388, 'type': 'word', 'position': 74}\n",
      "{'token': 'wish', 'start_offset': 389, 'end_offset': 395, 'type': 'word', 'position': 75}\n",
      "{'token': 'leave', 'start_offset': 399, 'end_offset': 404, 'type': 'word', 'position': 77}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_analyzer = analyzer('default',\n",
    "    type='custom',\n",
    "    tokenizer=tokenizer('letter'),\n",
    "    filter=['stop', 'kstem'] # snowball is stemming, erasing derivations\n",
    ")\n",
    "   \n",
    "ind = Index('foo', using=client)\n",
    "ind.close()\n",
    "ind.analyzer(my_analyzer)    \n",
    "ind.save()\n",
    "ind.open()\n",
    "\n",
    "res = ind.analyze(body={'analyzer':'default', 'text':u'We found my lady with no light in the room but the reading-lamp. The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat close at the table, and kept her eyes fixed obstinately on an open book. “Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes to leave it?”'})\n",
    "# cleanup...\n",
    "for r in res['tokens']:\n",
    "    print(r)\n",
    "ind.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The index reloaded\n",
    "\n",
    "The first task of this session is to study how the preprocessing pipeline changes how tokens are produced. You have a new version of last session’s indexer script named `IndexFilesPreprocess.py`.\n",
    "\n",
    "This new script has two additional flags `--token` and `--filter`.\n",
    "The flag `--token` changes the text tokenizer, and you have four options: `whitespace`, `classic`, `standard` and `letter`. \n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:** Use each one of them with the novels documents and compare the results. Do not change the filter that is used by default (in this case only lowercasing the string). Have a look into the documentation to understand what these tokenizers do. Use the `CountWords.py` script from last session to see how many tokens are obtained.\n",
    "\n",
    "After this, use the more aggressive tokenizer and use the filters available in the script: `lowercase` (obvious), `asciifolding` (gets rid of strange non ASCII characters that some languages love to use), `stop` (remove standard english stopwords) and the different stemming algorithms for the english language (`snowball`, `porter_stem` and `kstem`). You have to use the `--filter` flag, that must be the last one and you can put the filters to use separated by blank spaces, for instance:\n",
    "\n",
    "```\n",
    "$ python IndexFilesPreprocess.py --index news --path 20_newsgroups --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "Now you can answer the question, what word is the most frequent one in the English language (excluding stopwords)? (you will be surprised, or not, if you do this with the arxiv corpus)\n",
    "As a bonus, you can learn how to configure the text analyzer of an index and you can change the script so more options can be used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 33 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'nov': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'nov', 'creation_date': '1665906133732', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'asciifolding'], 'type': 'custom', 'tokenizer': 'letter'}}}, 'number_of_replicas': '1', 'uuid': 'Lec8UAxVTduwEwm9YoOpqA', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Indexing 33 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'nov': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'nov', 'creation_date': '1665906140205', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'asciifolding'], 'type': 'custom', 'tokenizer': 'whitespace'}}}, 'number_of_replicas': '1', 'uuid': 'YMsK6XV5TMiBAf8zVaPBdg', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Indexing 33 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'nov': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'nov', 'creation_date': '1665906148002', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'asciifolding'], 'type': 'custom', 'tokenizer': 'classic'}}}, 'number_of_replicas': '1', 'uuid': 'WEPuw-yEQWiYAcIhwcmR9A', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Indexing 33 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'nov': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'nov', 'creation_date': '1665906155040', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'asciifolding'], 'type': 'custom', 'tokenizer': 'standard'}}}, 'number_of_replicas': '1', 'uuid': 'SCHiLmYTR3Wzw-0G8Ma0Dw', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "!python IndexFilesPreprocess.py --index nov --path novels --token letter --filter lowercase asciifolding\n",
    "!python CountWords.py --index nov > novel_letter.csv\n",
    "!python IndexFilesPreprocess.py --index nov --path novels --token whitespace --filter lowercase asciifolding\n",
    "!python CountWords.py --index nov > novel_whitespace.csv\n",
    "!python IndexFilesPreprocess.py --index nov --path novels --token classic --filter lowercase asciifolding\n",
    "!python CountWords.py --index nov > novel_classic.csv\n",
    "!python IndexFilesPreprocess.py --index nov --path novels --token standard --filter lowercase asciifolding\n",
    "!python CountWords.py --index nov > novel_standard.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer letter has: 31859 Words\n",
      "The tokenizer whitespace has: 109723 Words\n",
      "The tokenizer classic has: 47322 Words\n",
      "The tokenizer standard has: 48495 Words\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('/home/valeria/Dades/Tercer/CAI/lab3/novel_letter.csv','r', encoding='utf-8') as df:\n",
    "    reader = csv.reader(df)\n",
    "    data = list(reader)\n",
    "\n",
    "print (\"The tokenizer letter has:\",data[-1][0])\n",
    "\n",
    "with open('/home/valeria/Dades/Tercer/CAI/lab3/novel_whitespace.csv','r', encoding='utf-8') as df:\n",
    "    reader = csv.reader(df)\n",
    "    data = list(reader)\n",
    "\n",
    "print (\"The tokenizer whitespace has:\",data[-1][0])\n",
    "\n",
    "with open('/home/valeria/Dades/Tercer/CAI/lab3/novel_classic.csv','r', encoding='utf-8') as df:\n",
    "    reader = csv.reader(df)\n",
    "    data = list(reader)\n",
    "\n",
    "print (\"The tokenizer classic has:\",data[-1][0])\n",
    "\n",
    "with open('/home/valeria/Dades/Tercer/CAI/lab3/novel_standard.csv','r', encoding='utf-8') as df:\n",
    "    reader = csv.reader(df)\n",
    "    data = list(reader)\n",
    "\n",
    "print (\"The tokenizer standard has:\",data[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 33 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'novels1': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'novels1', 'creation_date': '1665906309193', 'analysis': {'analyzer': {'default': {'filter': ['lowercase', 'asciifolding', 'stop', 'kstem'], 'type': 'custom', 'tokenizer': 'standard'}}}, 'number_of_replicas': '1', 'uuid': 'dl65iehvSH27Wose7Q4orA', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    }
   ],
   "source": [
    "!python IndexFilesPreprocess.py --index novels1 --path novels --token standard --filter lowercase asciifolding stop kstem\n",
    "!python CountWords.py --index novels1 > novels_filt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is the most frequent word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word  i has:\t 14394 repetitions\n",
      "The word  his has:\t 12980 repetitions\n",
      "The word  he has:\t 11735 repetitions\n",
      "The word  had has:\t 9742 repetitions\n",
      "The word  which has:\t 9107 repetitions\n",
      "The word  from has:\t 7432 repetitions\n",
      "The word  have has:\t 6707 repetitions\n",
      "The word  were has:\t 6222 repetitions\n",
      "The word  all has:\t 5896 repetitions\n",
      "The word  we has:\t 5851 repetitions\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('/home/valeria/Dades/Tercer/CAI/lab3/novels_filt.csv','r') as df:\n",
    "    reader = csv.reader(df)\n",
    "    data = list(reader)\n",
    "\n",
    "for word_rep in data[0:10]:\n",
    "    print (\"The word\",word_rep[1],\"has:\\t\",word_rep[0], \"repetitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter - breaks text into terms whenever it encounters a character which is not a letter\n",
    "Whitespace - breaks text into terms whenever it encounters a whitespace character.\n",
    "Classic - grammar based tokenizer that is good for English language documents. This tokenizer has heuristics for special treatment of acronyms, company names, email addresses, and internet host names (works for English)\n",
    "Standard -  provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29) and works well for most languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ff350ef70>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = []\n",
    "total = 0\n",
    "final = False\n",
    "\n",
    "with open(\"/home/valeria/Dades/Tercer/CAI/lab3/novels_filt.csv\", \"r\") as df:\n",
    "    for line in df:\n",
    "        line = line.rstrip()\n",
    "\n",
    "        if (line == \"--------------------\"):\n",
    "            final = True\n",
    "        else:\n",
    "            if (final):\n",
    "                lista = line.split(\" \")\n",
    "                total=int(lista[0])\n",
    "            else:\n",
    "                lista = line.split(\",\")\n",
    "                words.append(int(lista[0]))\n",
    "plt.plot(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Computing Tf-Idf and Cosine similarity\n",
    "\n",
    "This part of the session is to make sure we understand the tf-idf weight scheme for representing documents as vectors and the cosine similarity measure. We will complete a script that receives the paths of two files, obtains its ids from the index, computes the tf-idf vectors for the corresponding documents, optionally prints the vectors and finally computes their cosine similarity.\n",
    "\n",
    "The script `TFIDFViewer.py` has a set of incomplete functions to achieve this:\n",
    "\n",
    "- The main program follows the schema just explained\n",
    "\n",
    "- The `search_file_by_path` function returns the id of a document in the index (the path has to be the exact full path where the documents were when indexed, not just a filename).\n",
    "\n",
    "- The `document_term_vector` function returns two _lists of pairs_, the first one is (t, frequency of t in the document), the second one is (t, number of docs in the index that contain t). Both lists are alphabetically ordered by term.\n",
    "\n",
    "- The incomplete `toTFIDF` function that returns a list of pairs (term, weight) representing the document with the given docid. It:\n",
    "    1. First gets two lists with term document frequency and term index frequency.\n",
    "    2. Gets the number of documents in the index.\n",
    "    3. Then finally creates every pair (term, tf-idf) entry of the vector to be returned.\n",
    "    \n",
    "    Your task here is to complete the computations of the tf-idf value to fill this vector. You have all the ingredients ready, and you only have to apply the formulas explained in class.\n",
    "\n",
    "- The incomplete `normalize` function should compute the norm of the vector (square root of the sums of components squared) and divide the whole vector by it, so that the resulting vector has norm (i.e. length) 1. Complete this function.\n",
    "\n",
    "- The incomplete `print_term_weigth` vector prints one line for each entry in the given vector of the form (term, weight). Complete this function.\n",
    "\n",
    "- The incomplete `cosine_similarity` function can be implemented by first normalizing both arguments (if they are not already), then computing their inner product. Complete this function. **IMPORTANT:** _It must be an efficient implementation, with at most one scan of each vector. Use strongly that the vectors are sorted by term alphabetically._\n",
    "\n",
    "For computing the square root and log10 you can use the numpy library functions `log10` and `sqrt`. This library is already imported in the script as `np`.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3:** Complete all functions; test your implementation you have a set of documents inside the `doc` directory that correspond to the ones used in the theory slides examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 7 files\n",
      "Reading files ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: the default value for the ?wait_for_active_shards parameter will change from '0' to 'index-setting' in version 8; specify '?wait_for_active_shards=index-setting' to adopt the future default behaviour, or '?wait_for_active_shards=0' to preserve today's behaviour\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: [types removal] Using include_type_name in put mapping requests is deprecated. The parameter will be removed in the next major version.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Index settings= {'docs_index': {'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'docs_index', 'creation_date': '1665906443564', 'analysis': {'analyzer': {'default': {'filter': ['lowercase'], 'type': 'custom', 'tokenizer': 'standard'}}}, 'number_of_replicas': '1', 'uuid': 'WZZi0nC0SgafPhTO0hVaBA', 'version': {'created': '7170699'}}}}}\n",
      "Indexing ...\n",
      "/home/valeria/.local/lib/python3.8/site-packages/elasticsearch/connection/base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"TFIDFViewer.py\", line 183, in <module>\n",
      "    file1_id = search_file_by_path(client, index, file1)\n",
      "  File \"TFIDFViewer.py\", line 45, in search_file_by_path\n",
      "    raise NameError(f'File [{path}] not found')\n",
      "NameError: File [docs/1] not found\n"
     ]
    }
   ],
   "source": [
    "!python IndexFilesPreprocess.py --index docs_index --path docs\n",
    "!python TFIDFViewer.py --index docs_index --files docs/1 docs/2 > docs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Experimenting\n",
    "\n",
    "Once you are done with your program, try it out with the test collections from the previous sessions. First, test your implementation by computing the similarity of a file with itself (what should it give?).\n",
    "\n",
    "You can do all sorts of experiments, for example, are the documents of a specific subset of the corpus `20_newgroups` more similar among them that to other unrelated subset (e.g `alt.atheism` vs. `sci-space`)? Explore and use your imagination.\n",
    "\n",
    "A final question. Have you noticed that we are searching the documents using the _path name_? By default, all text fields are tokenized. Yet, if the path field had been tokenized, these searches would not succeed, right? So, what did we do differently when indexing the documents so that we can look for an exact match in the path field? Check the script.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 4:** Explain what experimentation you have done, and whether the results you get make any sense. Finally, answer the question above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rules of delivery\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session. \n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report with your results and thoughts. Make it at most 2 pages. Strive to summarize what new things you learned in this session. You are welcome to add conclusions and findings that depart from what we asked you to do.\n",
    "\n",
    "- Turn the report to PDF. Make sure it has your names, date, and title. Create a single `.zip` file all the python scripts that you created or modified; in the modified scripts, make sure you mark visibly with comments the parts that you modified.\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html). There will be a `Practica` open for each report.\n",
    "\n",
    "- Deadline: Work must be delivered __within 2 weeks__ from the end of the lab session. Late submissions risk being penalized or not accepted at all. If you anticipate problems with the deadline, tell us as soon as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "253bccbbe9164b37c09c8a287aff62cc626d7d2b598de3051709fae16766d7d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
